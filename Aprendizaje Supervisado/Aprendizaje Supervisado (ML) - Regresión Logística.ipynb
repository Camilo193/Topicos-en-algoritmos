{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje supervisado - Regresión Logística\n",
    "***\n",
    "\n",
    "La idea ahora, que ya tenemos nuestra matriz de datos/características pre-procesada, es iniciar nuestro recorrido por los algoritmos de clasificación. Para ello contaremos con una base de datos muy conocida en el ML llamada Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris\n",
    "***\n",
    "\n",
    "Esta base de datos continuene 50 muestras de 3 diferentes tipos de iris (150 muestras en total). Las características de cada una de las observaciones son: la longitud y el ancho del sépalo,  y la longitud y ancho del pétalo. El objetivo de ML con esta base de datos es predecir la especie de iris de acuerdo a sus medidas.\n",
    "\n",
    "Las especies de irir son: Iris Setosa, Iris Versicolour, Iris Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris](http://terpconnect.umd.edu/~petersd/666/html/iris_with_labels.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar la base de datos desde la librería scikit-learn utilizamos las siguientes instrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets #la librería sklearn contiene las funciones \n",
    "#relacionadas con aprendizaje de máquina. Datasets incorpora bases de datos de \n",
    "#ejemplo\n",
    "import numpy as np #importamos la librería numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris() #aquí cargamos los datos de la base de datos \n",
    "#en la variable iris.\n",
    "X = iris.data #guardamos en la variable X la matriz de observaciones\n",
    "y = iris.target #guardamos en la variable y el vector de etiquetas \n",
    "#(identifica cada fila de la matriz X con una clase específica de iris)\n",
    "print(np.unique(y)) #imprimimos los valores del vector de etiquetas y\n",
    "#que no se repitan\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística\n",
    "\n",
    "Es un algoritmo simple pero potente para problemas de clasificación lineales y binarios. A pesar del nombre no es un modelo de regresión, es un modelo de clasificación.\n",
    "\n",
    "La regresión logística es un modelo de clasificación muy simple de implementar y que funciona muy bien en clases linealmente separables. A nivel industrial es uno de los modelos más utilizados. \n",
    "\n",
    "Dado que es un clasificador binario, si se desea extender sus prestaciones a clasificación multiclase de debe utilizar la técnica OvR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiciones\n",
    "\n",
    "Algunas definiciones importantes para entender este tipo de clasificadores son:\n",
    "\n",
    "1. *Odds ratio o razón de momios:*\n",
    ">La razón de momios  indica la probabilidad a favor de un evento particular, y puede ser escrita como $\\frac{p}{1-p}$, donde $p$ denota la probabilidad del evento positivo. Siendo el evento positivo, el evento que queremos predecir.\n",
    "\n",
    "2. *Función logit:*\n",
    ">La función *logit* se define como el logaritmo de la razón de momios:\n",
    "\n",
    "$$logit(p)=\\log\\frac{p}{1-p}$$\n",
    "\n",
    " La función *logit* toma valores de entrada en el rango de 0 a 1 y los transofrma que números reales, con lo que se puede expresar la siguiente relación lineal entre las características y estos valores.\n",
    "\n",
    "$$logit(P(y=1|\\mathbf{x}))=w_0x_0+w_1x_1+\\ldots+w_px_p= \\sum_{j=0}^{p}w_jx_j=\\mathbf{w}^{\\top}\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionamiento\n",
    "\n",
    "La probabilidad de que cierta muestra pertenezca a una clase particular, que es el objeto de nuestro interés,  es la forma inversa de la función *logit*. Esta función es llamada *función logística* o *sigmoid*, y se define como:\n",
    "\n",
    "$$\\phi(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "En este caso $z$ es la red de entrada, i.e., la combinación lineal de l os pesos y las características, $z=\\mathbf{w}^{\\top}\\mathbf{x}$. \n",
    "\n",
    "La función sigmoide se verá de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #importamos la librería para graficar\n",
    "import math #importamos la librería que tiene las funciones matemáticas\n",
    "#avanzadas\n",
    "\n",
    "#ahora definiremos una función que represente el sigmoide\n",
    "def sigmoide(z): #definimos la función con nombre sigmoide. El argumento\n",
    "    #de entrada será la variable z\n",
    "    funcion_sigmoide = 1.0/(1.0+np.exp(-z)) #esta es la definición matemática \n",
    "    #de la función sigmoide\n",
    "    return funcion_sigmoide #retornamos la función evaluada en z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEiIikpFCQkREMlJIiIhIRv8fvwnpZsxXUoAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc7cb827b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#para probarla debemos generar una  malla de valores en z (una matriz)\n",
    "z = np.mgrid[-10:10:0.1]#-10 y 10 son unos valores para visualizar\n",
    "fi = sigmoide(z) #llamamos la función, la evaluamos en z y la almacenamos\n",
    "#en la variable fi\n",
    "plt.plot(z,fi) #graficamos z vs fi\n",
    "plt.axvline(0,0,color='k') #aquí graficamos una línea vertical que cruza por\n",
    "#(0,0) de color negro (k)\n",
    "plt.axhline(y=0.5, ls='dotted', color='k') #aquí graficamos una línea\n",
    "#horizontal que cruza por y=0.5, es punteada y de color negro\n",
    "plt.ylabel('$\\phi(z)$') #nombre del eje y. Los signos de $ es para que \n",
    "#se vea la variable phi\n",
    "plt.xlabel('z') #nombre del eje x\n",
    "plt.show() #mostramos la figura\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de la función *sigmoide* $\\phi(z) = P(y=1|\\mathbf{x};\\mathbf{w})$ puede ser interpretada como la probabilidad de una observación particual de pertenecer a la clase 1, dadas sus características $\\mathbf{x}$ parametrizadas por los pesos $\\mathbf{w}$.\n",
    "\n",
    "*Ejemplo*\n",
    "\n",
    "Si calculamos $\\phi = 0.8$ para una muestra  determinada en la base de datos Iris, significa que la probabilidad de que sea una Iris Versicolour es del 80%. Similarmente, para una Iris Setosa puede calcularse así: \n",
    "\n",
    "$$P(y=0|\\mathbf{x};\\mathbf{w})=1-P(y=1|\\mathbf{x};\\mathbf{w})=0.2 $$\n",
    "\n",
    "La probabilidad de  salida se puede convertir en una salida binaria a través de un cuantizador (una función de escalón unitario):\n",
    "\n",
    "$$ \\hat{y}=\\left\\{\\begin{array}{cc}\n",
    "1& \\phi(z)\\leq 0.5\\\\\n",
    "0& otros\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sintonización de parámetros\n",
    "\n",
    "Para la sintonización o el cálculo de los pesos $w$, se debe tener en cuenta la función de costo que se desea minimizar o maximizar. En nuestro caso la función de costo es el error cuadrático medio:\n",
    "\n",
    "$$J(\\mathbf{w})=\\frac{1}{2}\\sum_{i}(\\phi(z_i)-y_i)^2$$\n",
    "\n",
    "Este error corresponde a la diferencia que hay entre la clase estimada y la etiqueta real de cada observación. Ahora bien, la minimización de la función de costos implica la maximización de la función de verosimilitud, definida como:\n",
    "\n",
    "$$L(\\mathbf{w}) = P(\\mathbf{y}|\\mathbf{x};\\mathbf{w})=\\prod_{i=1}^{n}P(y_i|x_i;\\mathbf{w})=\\prod_{i=1}^{n}(\\phi(z_i))^{y_i}(1-\\phi(z_i))^{1-y_i}$$\n",
    "\n",
    "Dado que es más fácil maximizar el logaritmo de la función de verosimilitud, tenemos que:\n",
    "\n",
    "$$l(\\mathbf{w})=\\log(L(\\mathbf{w}))=\\sum_{i=1}^{n}[y_i\\log(\\phi(z_i))+(1-y_i)\\log(1-\\phi(z_i))]$$\n",
    "\n",
    "Ahora bien, podemos usar un algoritmo como gradiente ascendente para maximizar la función de verosimilitud o minimizarla a través de gradiente descendente. Para la minimización tendremos que la función de costos debe ser:\n",
    "\n",
    "$$J(\\mathbf{w})= \\sum_{i=1}^{n}[-y_i\\log(\\phi(z_i))-(1-y_i)\\log(1-\\phi(z_i))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n",
    "\n",
    "Para implementar nuestro clasificador, utilizaremos el modelo de la librería scikit-learn sobre la base de datos Iris. Primero debemos importar el clasificador y organizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #llamamos la librería\n",
    "#linear_model perteneciente a sklearn, donde se encuentran los modelos\n",
    "#lineales como Regresión logística. \n",
    "from sklearn.model_selection import train_test_split # con esta librería\n",
    "#podemos dividir nuestra base de datos en entrenamiento y validación\n",
    "\n",
    "\n",
    "#primero hacemos nuestra partición de la DB.\n",
    "# En este caso el 0.3 corresponde al 30% de las obervaciones de la\n",
    "#base de datos que se tomarán como test o validación, mientras el 70%\n",
    "#restante se tomará como entrenamiento (los datos con los que le enseñamos\n",
    "#a la máquina)\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.3, random_state=0)\n",
    "#print (X_train.shape)\n",
    "#print (X_test.shape)\n",
    "\n",
    "#print(length(y_train))\n",
    "#print(length(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1**\n",
    "\n",
    "Define y aplica una función para normalizar (preprocesar) el conjunto de entrenamiento y el conjunto de validación utilizando la media y la desviación estándar, de tal forma que :\n",
    "\n",
    "media_X_train = X_train.mean(axis = 0)\n",
    "\n",
    "desviacion_X_train = X_train.std(axis = 0)\n",
    "\n",
    "X_train_normalizado = (X_train - media_X_train)/desviacion_X_train\n",
    "\n",
    "X_test_normalizado  = (X_test -  media_X_train)/desviacion_X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#espacio para solucionar el ejercicio 1\n",
    "def Remocion(X): #definimos la función\n",
    "    X = X - X.mean(axis=0) # a los datos les restamos el valor de la media\n",
    "    X = X/X.std(axis=0) #al resultado lo dividimos por la desviación estándar\n",
    "    return X #retornamos los datos normalizamos\n",
    "\n",
    "def remocion_train_test(X_train,X_test):\n",
    "    media_train = X_train.mean(axis = 0) # calculamos la media del conjunto de entrenamiento\n",
    "    desviacion_train = X_train.std(axis = 0) # calculamos la desviacion estándar del conjunto de entrenamiento\n",
    "    \n",
    "    X_train_normalizado = (X_train - media_train)/desviacion_train # preprocesamos la matriz de entrenamiento\n",
    "    X_test_normalizado = (X_test - media_train)/desviacion_train # preprocesamos la matriz de validacion o test\n",
    "    \n",
    "    return X_train_normalizado, X_test_normalizado\n",
    "\n",
    "X_train_normalizado, X_test_normalizado = remocion_train_test(X_train, X_test)\n",
    "print(type(X_train_normalizado))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos el clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=0,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instanciamos el clasificador, valores pequeños de C especifican\n",
    "# mayor regularización\n",
    "clasificador = LogisticRegression(C=1000.0, random_state = 0) \n",
    "# entrenamos el clasificador con los datos X_train_normalizado\n",
    "clasificador.fit(X_train_normalizado,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las muestras mal clasificadas fueron 1\n"
     ]
    }
   ],
   "source": [
    "#Realizamos el test o la validación con los datos que el clasificador no \n",
    "#conoce, que corresponden al X_test_normalizado\n",
    "y_pred = clasificador.predict(X_test_normalizado)\n",
    "# comparamos las etiquetas de la base de datos con las que encontró\n",
    "# el clasificador para así fijar el error de clasificación\n",
    "print('Las muestras mal clasificadas fueron %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Repite el procedimiento anterior pero con la normalización del escalamiento\n",
    "\n",
    "$$\\frac{\\mathbf{X}-min(\\mathbf{X})}{max(\\mathbf{X})-min(\\mathbf{X})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las muestras mal clasificadas fueron 1\n"
     ]
    }
   ],
   "source": [
    "# espacio para solucionar el ejercicio 2\n",
    "def Escalamiento(X): #definimos la función de escalamiento\n",
    "    minimo = X.min(axis=0) #calculamos el mínimo valor de cada columna\n",
    "    maximo = X.max(axis=0) #calculamos el máximo valor de cada columna\n",
    "    X = (X-minimo)/(maximo-minimo) #aplicamos la fórmula del escalamiento\n",
    "    return(X) #retornamos el valor\n",
    "\n",
    "def escalamiento_train_test(X_train,X_test):\n",
    "    minimo_train = X_train.min(axis=0)\n",
    "    maximo_train = X_train.max(axis=0)\n",
    "    \n",
    "    X_train_preprocesado = (X_train - minimo_train)/(maximo_train - minimo_train)\n",
    "    X_test_preprocesado = (X_test - minimo_train)/(maximo_train - minimo_train)\n",
    "    \n",
    "    return X_train_preprocesado, X_test_preprocesado\n",
    "   \n",
    "X_train_preprocesado, X_test_preprocesado = escalamiento_train_test(X_train,X_test)\n",
    "\n",
    "#instanciamos el clasificador, valores pequeños de C especifican\n",
    "# mayor regularización\n",
    "clasificador = LogisticRegression(C=1000.0, random_state = 0) \n",
    "# entrenamos el clasificador con los datos X_train_normalizado\n",
    "clasificador.fit(X_train_preprocesado,y_train)\n",
    "\n",
    "#Realizamos el test o la validación con los datos que el clasificador no \n",
    "#conoce, que corresponden al X_test_normalizado\n",
    "y_pred = clasificador.predict(X_test_preprocesado)\n",
    "# comparamos las etiquetas de la base de datos con las que encontró\n",
    "# el clasificador para así fijar el error de clasificación\n",
    "print('Las muestras mal clasificadas fueron %d' % (y_test != y_pred).sum())\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Realiza un código para la comparación de los resultados de los dos preprocesamientos pero con respecto al porcentaje de acierto de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las muestras bien clasificadas fueron 44\n",
      "45\n",
      "97.7777777778\n"
     ]
    }
   ],
   "source": [
    "#espacio para solucionar el ejercicio 3\n",
    "print('Las muestras bien clasificadas fueron %d' % (y_test == y_pred).sum())\n",
    "print(len(y_test))\n",
    "print((100*(y_test == y_pred).sum())/len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
